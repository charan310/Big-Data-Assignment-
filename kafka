# I have created data locally "EVENT.csv"

# Start kafka service
kafka-console-producer.sh -bootstrap-server localhost:9092 --topic kevent

1.Connect local data to Kafka using PySpark job
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_json
from pyspark.sql.types import StructType, StringType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("LocalDataToKafka") \
    .getOrCreate()

# Read data from local CSV
data = spark.read.csv("mnt/d/EVENT.csv", header=True)

# Define schema for Kafka
kafka_schema = StructType() \
    .add("Event_Name", StringType()) \
    .add("Event_Type", StringType()) \
    .add("Event_Value", StringType()) \
    .add("Event_Timestamp", StringType()) \
    .add("Event_Page_Source", StringType()) \
    .add("Event_Page_URL", StringType()) \
    .add("Event_Component_ID", StringType()) \
    .add("User_ID", StringType()) \
    .add("Event_Date", StringType())

# Convert data to JSON and write to Kafka
data.select(to_json(data.schema.names).alias("value")) \
    .write \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "kevent") \
    .save()
